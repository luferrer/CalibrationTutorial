# This script is a template for evaluating calibration of the posteriors generated by a
# classification system. It also shows what you would need to add at the output of your classifier
# if turns out that it was badly calibrated.

# IMPORTANT: this script assumes the scores are log-posteriors NOT posteriors. This is to avoid
# numerical issues when taking the exponential. 

# See the the notebook 00_measure_perf_and_fix_calibration for a discussion on how to obtain the
# log-posteriors for various usual types of classifiers (DNN preactivations, LLRs, or SVMs).

import numpy as np
from expected_cost.data import create_scores_for_expts
from expected_cost.calibration import calibration_with_crossval, calibration_train_on_heldout
from expected_cost.psrcal_wrappers import Brier, LogLoss
from psrcal.calibration import AffineCalLogLoss, AffineCalBrier, HistogramBinningCal

###################################################################################################
# Load the data

# For this example, we create some simulated data. You can change this section to load your own data
# as long as in the end you have defined scores_tst and target_tst, and, optionally, scores_trn and
# targets_trn and condition_ids. 
# 
# If you are doing cross-validation, you do not need the two trn variables.
# 
# If you are doing train-on-test calibration, you should set scores_trn=scores_tst and
# targets_trn=targets_tst. 
# 
# The condition_ids variable should be set to the indices of the conditions that might result in
# correlations between samples (eg, the speaker id or the microphone type in speech data). If the
# samples are assumed to be iid (no correlation-inducing factors are known), then that variable
# should be set to None. 

P0 = 0.9
N = 10000
K = 2
std = 0.15
score_dict, targets = create_scores_for_expts(K, P0=P0, feat_std=0.15, N=N)
scores = score_dict['mc2']['Mismp'] 

shuf = np.arange(len(targets)-1)
np.random.shuffle(shuf)
scores = scores[shuf]
targets = targets[shuf]

N_tst = int(N/2)
scores_tst = scores[:N_tst]
scores_trn = scores[N_tst:]

targets_tst = targets[:N_tst]
targets_trn = targets[N_tst:]

condition_ids = None

###################################################################################################
# Create the calibrated scores and the calibration model

# Choose EPSR (LogLoss or Brier), calibration method, and, optionally, set the priors to the
# deployment ones, if they are expected to be different from the ones in the test data.

metric = LogLoss 
calmethod = AffineCalLogLoss
deploy_priors = None
calparams = {'bias': True, 'priors': deploy_priors}


if scores_trn is not None:
    # If you have training data data (held-out or the same as the test data for train-on-test)

    scores_tst_cal, calmodel = calibration_train_on_heldout(scores_tst, scores_trn, targets_trn, 
                                                            calparams=calparams, 
                                                            calmethod=calmethod, 
                                                            return_model=True)

else:
    # Alternatively, if all you have is test data, first do cross-validation to get the calibration
    # loss, and then retrain the calibration model on all that data. This is the calibration model you
    # would deploy if you find out that calibration loss is large.

    scores_tst_cal = calibration_with_crossval(scores_tst, targets_tst, 
                                            calparams=calparams, 
                                            calmethod=calmethod)

    _, calmodel = calibration_train_on_heldout(scores_tst, scores_tst, targets_tst, 
                                               calparams=calparams, 
                                               calmethod=calmethod, 
                                               return_model=True)


###################################################################################################
# Compute the selected EPSR before and after calibration

overall_perf = metric(scores_tst, targets_tst, priors=deploy_priors, norm=True)
overall_perf_after_cal = metric(scores_tst_cal, targets_tst, priors=deploy_priors, norm=True)
cal_loss = overall_perf-overall_perf_after_cal
rel_cal_loss = 100*cal_loss/overall_perf

print(f"Overall performance before calibration ({metric.__name__}) = {overall_perf:4.2f}" ) 
print(f"Overall performance after calibration ({metric.__name__}) = {overall_perf_after_cal:4.2f}" ) 
print(f"Calibration loss = {cal_loss:4.2f}" ) 
print(f"Relative calibration loss = {rel_cal_loss:4.1f}%" ) 

###################################################################################################
# Use the calibration model trained above on new data

# If you find that the relative calibration loss is large, this means your system needs a calibration
# stage. All you need to do is use the calmodel above. Here is how to use it:

# new_scores_cal = calibrate_scores(new_scores, calmodel)

# Remember that you need to first make that new_scores are log-posteriors, as explained in the 
# notebook mentioned above.